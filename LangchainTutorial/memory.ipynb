{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:35:00.738656Z",
     "start_time": "2024-02-22T13:35:00.733435Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp, GPT4All\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "n_gpu_layers = 1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 1024  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/francescobassignana/models/meta-models/llama-2-13b-chat/13b-llama-ggml-model-q4_0.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    max_tokens=2048,\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec6f880990cdb83e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "path = \"/Users/francescobassignana/Library/Application Support/nomic.ai/GPT4All/mistral-7b-openorca.Q4_0.gguf\"\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=path,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:35:04.955257Z",
     "start_time": "2024-02-22T13:35:04.725892Z"
    }
   },
   "id": "e96cc3d00f7e4ae0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SummaryMemory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c306cbfb1c4815cc"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:35:07.347705Z",
     "start_time": "2024-02-22T13:35:07.329571Z"
    }
   },
   "id": "366db540be919a52"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BufferMemory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "748ebc1a8b7b6487"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc44c4254a8c1eb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BufferWindowMemory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3989dc77cc893836"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "#only keep the last 2 iterations\n",
    "memory = ConversationBufferWindowMemory(k=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:34:37.226229Z",
     "start_time": "2024-02-22T13:34:37.220916Z"
    }
   },
   "id": "4d0f7cd7cfc36f78"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ConversationSummaryBufferMemory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1be54ed58d09c59"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# k = 2 only keep the last 2 iterations\n",
    "# max_token_limits = 2048 only keep the last 2048 tokens\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limits=40, k=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:33:47.166934Z",
     "start_time": "2024-02-22T13:33:47.161542Z"
    }
   },
   "id": "2a0bd2db880ce744"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "template = \"\"\"\n",
    "        [INST]\n",
    "        <SYS_PROMPT>\n",
    "            You are a knowledgeable assistant using GPT technology, skilled across many topics. Provide clear, informative, and relevant responses to user queries, engaging professionally and asking for clarification when needed.\n",
    "\n",
    "            Do not offer personal opinions or unverified information. If unsure, admit it. Avoid unsafe, disrespectful, or sensitive content, adhering to guidelines and privacy without engaging in real-time updates or transactions.\n",
    "            \n",
    "            Respond to user preferences for brevity or detail, and do not provide additional information without user prompts.\n",
    "        </SYS_PROMPT>\n",
    "        Current conversation:\n",
    "        {history}\n",
    "        Friend: {input}\n",
    "        AI Assistant:\n",
    "        [/INST]\n",
    "        \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"history\", \"input\"])\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True, prompt=prompt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:35:12.254829Z",
     "start_time": "2024-02-22T13:35:11.801124Z"
    }
   },
   "id": "202c7d430e0155ca"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3m\n",
      "        [INST]\n",
      "        <SYS_PROMPT>\n",
      "            You are a knowledgeable assistant using GPT technology, skilled across many topics. Provide clear, informative, and relevant responses to user queries, engaging professionally and asking for clarification when needed.\n",
      "\n",
      "            Do not offer personal opinions or unverified information. If unsure, admit it. Avoid unsafe, disrespectful, or sensitive content, adhering to guidelines and privacy without engaging in real-time updates or transactions.\n",
      "            \n",
      "            Respond to user preferences for brevity or detail, and do not provide additional information without user prompts.\n",
      "        </SYS_PROMPT>\n",
      "        Current conversation:\n",
      "        \n",
      "        Friend: Hello, I'm Sam\n",
      "        AI Assistant:\n",
      "        [/INST]\n",
      "        \u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"\\n        Hi there! It's great to meet you, Sam. If you have any questions or need assistance with anything, feel free to ask and I will be more than happy to help.\""
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hello, I'm Sam\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T13:35:21.914138Z",
     "start_time": "2024-02-22T13:35:12.715729Z"
    }
   },
   "id": "ebf054194ff2b95a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation.predict(input=\"can you help me with some customer support?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2da07e0fd07d6c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation.predict(input=\"My tv is not working\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd043ccb79d02d18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(conversation.memory.buffer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f79e7a2f3863a182"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Thank you for your help, can you remember what's my name?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b21e5e2b4b19fe7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8457a4fd3e2d6a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
